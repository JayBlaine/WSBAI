{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WSBAI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayBlaine/WSBAI/blob/main/WSBAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwJMdWluSex"
      },
      "source": [
        "Regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDkeV59TotXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a9beb50-2c66-4620-b1cd-85e4f83518c5"
      },
      "source": [
        "#!apt-get -qq install -y libfluidsynth1\n",
        "#!pip install numpy\n",
        "#!pip install pandas\n",
        "#!pip install yfinance\n",
        "#!pip install tensorflow\n",
        "#!pip install scikit-learn\n",
        "#!pip install praw\n",
        "#!pip install asyncpraw\n",
        "#!pip install datetime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datetime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/22/a5297f3a1f92468cc737f8ce7ba6e5f245fcfafeae810ba37bd1039ea01c/DateTime-4.3-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.5MB/s \n",
            "\u001b[?25hCollecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/57/8a68360d697cf9159cba5ee35f2d25bdcda33883e8b5a997714a191a0b11/zope.interface-5.3.0-cp37-cp37m-manylinux2010_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from datetime) (2018.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface->datetime) (54.1.2)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-4.3 zope.interface-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHeHu1uERD29"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-hcxPujnnCi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_datareader as web\n",
        "import datetime as dt\n",
        "import csv\n",
        "import os\n",
        "\n",
        "def main():\n",
        "    stonk = \"GME\"\n",
        "    flairs = {\"Gain\", \"Loss\", \"YOLO\"}\n",
        "    start = dt.datetime(2021, 1, 1)\n",
        "    end = dt.datetime.today()\n",
        "\n",
        "    data = web.DataReader(stonk,'stooq', start, end).reset_index()\n",
        "    data.drop('Open', 1, inplace=True)\n",
        "    data.drop('High', 1, inplace=True)\n",
        "    data.drop('Low', 1, inplace=True)\n",
        "\n",
        "    if not os.path.exists('data'):\n",
        "        os.makedirs('data')\n",
        "\n",
        "    data = data.to_csv(r'./data/stonkdata.csv', index=False, header=True)\n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEyY5U8DUgGM"
      },
      "source": [
        "# WSB data set read\n",
        "# DO NOT RUN AFTER LIVE SCRAPE STARTS\n",
        "# SERIOUSLY, YOU WILL RUIN DATASET ONCE LIVE SCRAPING BEGINS\n",
        "\n",
        "import pandas as pd\n",
        "import csv\n",
        "import datetime as dt\n",
        "\n",
        "def gmeChecker(title, body):\n",
        "  gmeKeys = ['gme', 'gamestop', 'dfv', 'deep fucking value', 'citadel', 'ken griffin', 'keith', 'gill', 'roaring kitty', 'if he\\'s in']\n",
        "  title = title.strip().lower()\n",
        "  body = body.strip().lower()\n",
        "  for key in gmeKeys:\n",
        "    # body and title\n",
        "    if key in title or key in body:\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def dateChecker(date1):\n",
        "\n",
        "  dtTemp = dt.datetime.strptime(date1, '%Y-%m-%d %H:%M:%S')\n",
        "  actDate = dtTemp.date()\n",
        "\n",
        "  return actDate\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "# date, reddit total posts, gme total posts, ratio of gme to total, \n",
        "# avg score of reddit posts, average score of gme posts, avg comments total, avg comments gme\n",
        "redditData = [] # 8 values, 2d list\n",
        "tempCSVHolder = []\n",
        "byDate = {}\n",
        "\n",
        "with open('reddit_wsb.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    line_count = 0\n",
        "    for row in csv_reader:\n",
        "\n",
        "      if line_count > 0:\n",
        "        tempTitle = row[0]\n",
        "        tempScore = row[1]\n",
        "        tempCommNum = row[4]\n",
        "        tempBody = row[6]\n",
        "        tempDate = row[7]\n",
        "        tempPass = [tempTitle, tempScore, tempCommNum, tempBody, tempDate]\n",
        "        #tempCSVHolder.append(tempPass)\n",
        "\n",
        "        #tester = gmeChecker(tempTitle, tempBody)\n",
        "        date1 = dateChecker(tempDate)\n",
        "\n",
        "        if date1 not in byDate.keys():\n",
        "          byDate[date1] = []\n",
        "          byDate[date1].append(tempPass)\n",
        "          # print(byDate[date1][0][4])\n",
        "        \n",
        "        else:\n",
        "          byDate[date1].append(tempPass)\n",
        "\n",
        "      line_count+=1\n",
        "\n",
        "redditPrelim = []\n",
        "\n",
        "for key in byDate.keys():\n",
        "\n",
        "  if key.year < 2021: #Gets rid of rogue row at 9/2020\n",
        "      continue\n",
        "\n",
        "  i=0\n",
        "  gmeCnt = 0\n",
        "  gmeCommTotal = 0\n",
        "  gmeUpTotal = 0\n",
        "  while i < len(byDate[key]):\n",
        "\n",
        "    \n",
        "    gmeCheck = gmeChecker(byDate[key][i][0], byDate[key][i][3])\n",
        "    if gmeCheck:\n",
        "      gmeCnt+=1\n",
        "      gmeCommTotal+=int(byDate[key][i][2])\n",
        "      gmeUpTotal+=int(byDate[key][i][1])\n",
        "\n",
        "    #TODO\n",
        "    i+=1\n",
        "  if i > 0 and gmeCnt > 0: # both posts and gme posts\n",
        "    redditPrelim.append([key, i, gmeCnt, gmeCnt/i, gmeUpTotal/gmeCnt, gmeCommTotal/gmeCnt])\n",
        "  elif i > 0: # no gme posts\n",
        "    redditPrelim.append([key, i, gmeCnt, gmeCnt/i, 0, 0])\n",
        "  else: # no posts\n",
        "    continue\n",
        "\n",
        "  #print(gmeCnt)\n",
        "  #print(i)\n",
        "#print(redditPrelim)\n",
        "\n",
        "\n",
        "with open('wsb_clean.csv', mode='w') as write_file:\n",
        "    writer = csv.writer(write_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "    i=0\n",
        "    #TODO\n",
        "    writer.writerow(['Date', 'Total Posts', 'GME Count', '% GME', 'Avg GME Upvotes', 'Avg GME Comments'])\n",
        "    while i < len(redditPrelim):\n",
        "      writer.writerow([redditPrelim[i][0], redditPrelim[i][1], redditPrelim[i][2], redditPrelim[i][3], redditPrelim[i][4], redditPrelim[i][5]])\n",
        "      i+=1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SMNrlumo_jn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8fcd103-e35f-4824-fa0c-0143964df333"
      },
      "source": [
        "# REDDIT DATA SCRAPE\n",
        "import csv\n",
        "import pandas as pd\n",
        "import asyncpraw\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def main():\n",
        "  reddit = asyncpraw.Reddit(client_id='ORNE1J4ncT_tGg', client_secret= 'sa-2SPPkOY-w_kMIBSeAostYFg804Q', user_agent='WSBAI:Python/urllib:v0.0.1 (by u/WSBAI_00123)', \n",
        "                            username=\"WSBAI_00123\", password=\"lint$h@d0w,./\")\n",
        "  wsb_sub = await reddit.subreddit('WallStreetBets')\n",
        "\n",
        "  \n",
        "  i = 0\n",
        "  print(reddit.read_only)\n",
        "  posts = [] # posts that are good from keywords\n",
        "  keywords = ['GME', 'gamestop', 'diamond', 'diamond hands', 'ape', 'apes']\n",
        "  postIds = {}\n",
        "\n",
        "  # TODO: \n",
        "\n",
        "  # -Keyword list for relevant posts/comments\n",
        "  # -handle emojis\n",
        "  # -post id dictionary for posts already read through\n",
        "  # -possible CV of reddit posts if jpg/png\n",
        "  # -store into csv file\n",
        "\n",
        "  async for submission in wsb_sub.new(limit=20):\n",
        "    test = submission.downvote()\n",
        "    # submission.id\n",
        "    # submission.created_utc   CHECK FOR LESS THAN DAY OLD\n",
        "\n",
        "    # submission.title\n",
        "    # submission.num_comments\n",
        "    # submission.upvote_ratio\n",
        "    # submission.link_flair_text\n",
        "    # num text keywords\n",
        "    # num comment keywords\n",
        "\n",
        "\n",
        "    # submission.url           POSSIBLE IMG CHECKING\n",
        "    \n",
        "    # submission.score   ----MAY NOT NEED----\n",
        "    \n",
        "    \n",
        "    #---STUFF TO LOOP THROUGH TO COUNT NUM OF KEYWORDS--- comments, selftext\n",
        "    #submission.selftext\n",
        "    #comments = await submission.comments()\n",
        "    #for comment in comments:\n",
        "    #  print(comment.body)\n",
        "    #  print(comment.score)\n",
        "\n",
        "    #posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
        "  #posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  loop = asyncio.get_event_loop() \n",
        "  loop.run_until_complete(main())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "\n",
            "https://v.redd.it/aim61y5xz9p61\n",
            "\n",
            "https://i.redd.it/omvy6g8qz9p61.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: coroutine 'VotableMixin.downvote' was never awaited\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Retail Investors deserve the right to trade in a free & equal market, without manipulation. \n",
            "\n",
            "Time to move on from the SEC & contact FINRA regarding market manipulation. See [Finra.org](https://Finra.org) \\- \n",
            "\n",
            "Dark Pool transactions are still kept accountable, which means there are audit records for all trades. FINRA is the keeper of these records.\n",
            "\n",
            "[https://www.finra.org/rules-guidance/notices/18-25](https://www.finra.org/rules-guidance/notices/18-25)\n",
            "\n",
            "**Regulatory Notice 18-25** \n",
            "\n",
            "FINRA is issuing this *Notice* to remind Alternative Trading Systems (ATSs) of their supervision obligations.1 As registered broker-dealers and FINRA members, ATSs—like other broker-dealer trading platforms—are required to maintain supervisory systems that are reasonably designed to achieve compliance with applicable securities laws and regulations, and with applicable FINRA rules, including, for example, rules on disruptive or manipulative quoting and trading activity.\n",
            "\n",
            "[https://www.finra.org/rules-guidance/notices/15-09](https://www.finra.org/rules-guidance/notices/15-09)\n",
            "\n",
            " •   **FINRA** **Rule 5210** **(Publication of Transactions and Quotations):** Rule 5210 provides that \"no member shall publish or circulate, or cause to be published or circulated, any ...communication of any kind which purports to report any transaction as a purchase or sale of any security unless such member believes that such transaction was a bona fide purchase or sale of such security; or which purports to quote the bid price or asked price for any security, unless such member believes that such quotation represents a bona fide bid for, or offer of, such security.\" This rule prohibits activities such as fictitious quoting, spoofing and layering of quotes. In addition, Supplementary Material .02 to Rule 5210 requires firms to adopt policies and procedures regarding \"self-trades,\" which are defined as \"transactions in a security resulting from the unintentional interaction of orders originating from the same firm that involve no change in the beneficial ownership of the security.\"4 Under Rule 5210 and Supplementary Material .02, firms must have policies and procedures in place that are reasonably designed to review their trading activity for, and prevent, a pattern or practice of self-trades resulting from orders originating from a single algorithm or trading desk or from related algorithms or trading desks. Self-trades resulting from orders that originate from unrelated algorithms or separate and distinct trading strategies within the same firm are generally considered to be bona fide transactions. \n",
            "\n",
            "•   FINRA Rule 6140 (Other Trading Practices) Rule 6140 contains several provisions that were adopted to ensure the promptness, accuracy and completeness of last sale information and to prevent that information from being publicly trade reported in a fraudulent or manipulative manner. For example, Rule 6140(a) prohibits a firm from executing purchases of NMS stocks at successively higher prices (or sales at successively lower prices) for the purpose of creating or inducing a false, misleading or artificial appearance of activity in such security; unduly or improperly influencing the market price for such security; or establishing a price that does not reflect the true state of the market for such security.\n",
            "\n",
            "\n",
            "FILE A COMPLAINT HERE: [https://www.finra.org/complaint/form](https://www.finra.org/complaint/form)\n",
            "https://www.reddit.com/r/wallstreetbets/comments/mddj2e/not_just_for_our_generation_its_for_the_next/\n",
            "\n",
            "https://i.redd.it/yts1g30qy9p61.jpg\n",
            "\n",
            "https://v.redd.it/w696lcbtx9p61\n",
            "\n",
            "https://i.redd.it/qdcfqovjt9p61.jpg\n",
            " Yo, listen up here's a story  \n",
            "About a little guy  \n",
            "That lives in a blue world  \n",
            "And all day and all night  \n",
            "And everything he sees is just blue  \n",
            "Like him inside and outside\n",
            "\n",
            "....... Just kidding!\n",
            "\n",
            "BLUEBIRD BIO $BLUE\n",
            "\n",
            "* Used to trade above $50 a month ago. Discrepancy in their sickle cell disease trial results caused 50% downfall from $50 to $25 overnight – was heavily shorted.\n",
            "* BLUE clarified the outcome of the trial results a week later - no issues. Consequently, analysts raised the target price to $70 (see article). Currently trading at $30.\n",
            "* Huge catalyst – Possible FDA approval of their breakthrough KarMMa for the treatment of multiple myeloma in collaboration with Bristol-Myers Squibb this week/end (3/27/21).\n",
            "* BLUE could easily fill the gap and jump from $30 to $50 on the FDA approval announcement. Highly underrated stock. Strong pipeline – most products in Phase 3 trials.\n",
            "* Hot sector – gene therapy and cancer treatment - CAR T Cell Therapy - $10+ billion market\n",
            "* Moreover, BLUE was mentioned in ARK Disrupt’s newsletter last week pertaining to research and medical advancements in gene therapy. Imagine the attention BLUE could get if ARK Genomics decides to invest in BLUE after their FDA approval for multiple myeloma treatment.\n",
            "\n",
            "Thoughts?\n",
            "\n",
            "Links:\n",
            "\n",
            "[https://www.marketwatch.com/press-release/car-t-cell-therapy-market-size-2021-by-consumption-volume-average-price-revenue-market-share-and-trend-to-2025-2021-03-17](https://www.marketwatch.com/press-release/car-t-cell-therapy-market-size-2021-by-consumption-volume-average-price-revenue-market-share-and-trend-to-2025-2021-03-17)\n",
            "\n",
            "[https://www.webull.com/news/39559197](https://www.webull.com/news/39559197)\n",
            "\n",
            "[https://www.webull.com/news/39896495](https://www.webull.com/news/39896495)\n",
            "https://www.reddit.com/r/wallstreetbets/comments/mdcvns/bluebird_bio/\n",
            "\n",
            "https://i.redd.it/3hfxznxxr9p61.jpg\n",
            "\n",
            "https://v.redd.it/623tw0xzm9p61\n",
            "\n",
            "https://finance.yahoo.com/news/annaly-capital-management-inc-announces-200500749.html\n",
            "\n",
            "https://i.redd.it/byw2ymafm9p61.jpg\n",
            "\n",
            "https://i.redd.it/t3al96ytk9p61.jpg\n",
            "\n",
            "https://finance.yahoo.com/news/gamestop-gme-upgraded-buy-does-160004421.html\n",
            "\n",
            "https://v.redd.it/d9bpzf9ej9p61\n",
            "\n",
            "https://i.redd.it/56xrephzf9p61.jpg\n",
            "\n",
            "https://i.redd.it/ts4qm6tfe9p61.jpg\n",
            "\n",
            "https://www.reddit.com/gallery/mdbbqi\n",
            "\n",
            "https://www.reddit.com/gallery/mdamgf\n",
            "\n",
            "https://i.redd.it/hgfr3l2u59p61.png\n",
            "\n",
            "https://www.reddit.com/gallery/mdaczb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/asyncio/tasks.py:249: RuntimeWarning: coroutine 'VotableMixin.downvote' was never awaited\n",
            "  result = coro.send(None)\n",
            "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqIZGgGFxK6C",
        "outputId": "f8c27e88-e38c-4153-9277-5859d37f44c7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "print(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}